{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e429146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hazm.utils import stopwords_list\n",
    "from hazm import WordTokenizer, Lemmatizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from minisom import MiniSom\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def convert_to_base(number_list, num_clusters):\n",
    "    converted_numbers = []\n",
    "    for number in number_list:\n",
    "        converted_number = number[0] * 10 + number[1] + 1\n",
    "        converted_numbers.append((converted_number - 1) % num_clusters + 1)\n",
    "    \n",
    "    return converted_numbers\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    tokenizer = WordTokenizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    stop_words = stopwords_list()\n",
    "    preprocessed_docs = []\n",
    "\n",
    "    batch_size = 100  # Number of documents to process in each batch\n",
    "\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "\n",
    "        # Tokenize, lemmatize, and remove stopwords for each document in the batch\n",
    "        batch_tokens = []\n",
    "        for doc in batch_docs:\n",
    "            tokens = tokenizer.tokenize(doc)\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            tokens = [token for token in tokens if token not in stop_words]\n",
    "            text_clean = ' '.join(tokens)\n",
    "            batch_tokens.append(text_clean)\n",
    "\n",
    "        preprocessed_docs.extend(batch_tokens)\n",
    "\n",
    "    return preprocessed_docs\n",
    "\n",
    "def generate_cluster_labels(filename):\n",
    "    # Read the Excel file and extract the documents\n",
    "    df = pd.read_excel(filename, sheet_name='Sheet')\n",
    "    docs = df['متن'].to_list()\n",
    "    title = df['عنوان'].drop_duplicates().tolist()\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    docs_train, docs_test = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Preprocess the training and test documents\n",
    "    docs_train = preprocess_documents(docs_train)\n",
    "    docs_test = preprocess_documents(docs_test)\n",
    "\n",
    "    # Convert documents to TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = vectorizer.fit_transform(docs_train).toarray()\n",
    "\n",
    "    # Perform PCA on the TF-IDF vectors\n",
    "    pca = PCA(n_components=2)\n",
    "    docs_vector = pca.fit_transform(tfidf_vectors)\n",
    "\n",
    "    # Create SOM instance\n",
    "    map_size = (10, 10)\n",
    "    input_dim = docs_vector.shape[1]\n",
    "    num_clusters = 12\n",
    "    som = MiniSom(map_size[0], map_size[1], input_dim, sigma=0.3, learning_rate=0.5)\n",
    "\n",
    "    # Initialize the weights using random values\n",
    "    som.random_weights_init(docs_vector)\n",
    "\n",
    "    # Train the SOM on the document vectors\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        som.train_random(docs_vector, num_iterations=len(docs_vector))\n",
    "\n",
    "    # Get the U-matrix\n",
    "    u_matrix = som.distance_map()\n",
    "\n",
    "    # Preprocess the test documents\n",
    "    docs_test = preprocess_documents(docs_test)\n",
    "\n",
    "    # Convert test documents to TF-IDF vectors\n",
    "    test_tfidf_vectors = vectorizer.transform(docs_test).toarray()\n",
    "    test_docs_vector = pca.transform(test_tfidf_vectors)\n",
    "\n",
    "    # Cluster the test documents using the trained SOM\n",
    "    cluster_labels = []\n",
    "    for doc_vector in test_docs_vector:\n",
    "        winner = som.winner(doc_vector)\n",
    "        cluster_labels.append(winner)\n",
    "\n",
    "    # Convert cluster indices to cluster labels between 1 and 12\n",
    "    cluster_labels = convert_to_base(cluster_labels, num_clusters)\n",
    "\n",
    "    # Write the cluster labels to the output text file\n",
    "    output_file_path = \"output.txt\"\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "        for label, doc in zip(cluster_labels, docs_test):\n",
    "            file.write(f\"Cluster {label}: {doc}\\n\")\n",
    "\n",
    "    print(\"Output has been saved to\", output_file_path)\n",
    "\n",
    "    # Save test_docs_vector to a text file\n",
    "    np.savetxt(\"test_docs_vector.txt\", test_docs_vector)\n",
    "\n",
    "    return test_docs_vector, cluster_labels, u_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c666ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of clusters\n",
    "num_clusters = 12\n",
    "\n",
    "# Calculate the approximate dimensions of the SOM map\n",
    "# You can adjust these values based on your specific requirements\n",
    "som_width = int(np.sqrt(num_clusters) * 2)\n",
    "som_height = int(np.sqrt(num_clusters) / 2)\n",
    "\n",
    "# Alternatively, you can directly set the som_width and som_height values\n",
    "# based on your preferred dimensions\n",
    "\n",
    "# Example:\n",
    "som_width = 6\n",
    "som_height = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c12d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
